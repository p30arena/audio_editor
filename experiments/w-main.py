# -*- coding: utf-8 -*-
"""VoiceEditing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GRyJ_YlD8dvqqfAbQvE45dwnrVl_oXd_
"""

#!python --version

# Install Whisper and WhisperX
#!pip install -q git+https://github.com/openai/whisper.git
#!pip install -q git+https://github.com/m-bain/whisperx.git

# Install Whisper and WhisperX
#!pip install -q git+https://github.com/openai/whisper.git
#!pip install -q git+https://github.com/federicotorrielli/BetterWhisperX


# Install other required libraries
#!pip install -q pydub webrtcvad soundfile librosa transformers jiwer

# Install FFmpeg
#!apt-get install -qq ffmpeg

# Install pyannote.audio for speaker diarization (optional, if needed)
#!pip install -q pyannote.audio

# Install Torch (if not already installed)
import torch
#if torch.__version__ < "1.7.0":
#    !pip install -q torch

#from google.colab import files

#uploaded = files.upload()

# This will prompt you to upload the file.
# After uploading, the file is saved in the current working directory.

#from google.colab import drive
#drive.mount('/content/drive')

#!cp "drive/MyDrive/audio_2024-11-09_16-32-11.ogg" "audio_2024-11-09_16-32-11.ogg"

# Now, you can access files in your Drive via /content/drive/MyDrive/
#!cp "audio_2024-11-09_16-32-11.ogg" "drive/MyDrive/audio_2024-11-09_16-32-11.ogg"

# pip install openai
# pip install python-dotenv
# pip install pydantic

# Import necessary libraries
from dotenv import load_dotenv
import json
import whisper
import webrtcvad
import wave
import contextlib
import numpy as np
from pydub import AudioSegment
import torch
import os
import librosa
import soundfile as sf

from pydantic import BaseModel
from openai import OpenAI

load_dotenv()
client = OpenAI()

def flatten(xss):
    return [x for xs in xss for x in xs]


# Step 1: Transcribe Audio
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the Whisper model
model = whisper.load_model("turbo", device)

# Path to your audio file
# audio_file = "audio_2024-11-09_16-32-11.ogg"
audio_file = "audio_2024-11-12_06-41-32.ogg"


# Step 2: Detect Non-Speech Segments
def vad_segments(audio_file, aggressiveness=3):
    vad = webrtcvad.Vad(aggressiveness)
    audio, sample_rate = librosa.load(audio_file, sr=16000)
    max_offset = len(audio)
    frame_duration = 30  # ms
    frame_size = int(sample_rate * frame_duration / 1000)
    frames = librosa.util.frame(audio, frame_length=frame_size, hop_length=frame_size).T
    segments = []
    offset = 0
    for frame in frames:
        is_speech = vad.is_speech((frame * 32767).astype(np.int16).tobytes(), sample_rate)
        if not is_speech:
            time_start = offset / sample_rate
            time_end = (offset + frame_size) / sample_rate
            segments.append((time_start, time_end))
        offset += frame_size
    return segments


print("Detecting non-speech segments...")
non_speech_segments = vad_segments(audio_file)
print("Detection complete.")

aggregated_segments = []
if non_speech_segments:
    # Initialize the first segment
    start_time = non_speech_segments[0][0]
    end_time = non_speech_segments[0][1]

    for current_start, current_end in non_speech_segments[1:]:
        if current_start <= end_time + 0.001:  # Allow small overlap/tolerance
            # Frames are consecutive or overlapping; extend the segment
            end_time = current_end
        else:
            # Gap between frames; save the current segment and start a new one
            aggregated_segments.append((start_time, end_time))
            start_time = current_start
            end_time = current_end
    # Add the last segment
    aggregated_segments.append((start_time, end_time))

non_speech_segments = []
for start_time, end_time in aggregated_segments:
    duration = end_time - start_time
    if duration >= 1.0:
        non_speech_segments.append({
            'start': start_time,
            'end': end_time,
            'text': '[Non-speech sound]',
            'type': 'NonSpeech'
        })

audio = AudioSegment.from_file(audio_file)
segments_to_keep = []
last_end = 0

for section in non_speech_segments:
    start_ms = int(section['start'] * 1000)
    end_ms = int(section['end'] * 1000)
    if start_ms > last_end:
        segments_to_keep.append(audio[last_end:start_ms])
    last_end = end_ms

if last_end < len(audio):
    segments_to_keep.append(audio[last_end:])

edited_audio = sum(segments_to_keep)
edited_audio.export("edited_non_speech_audio.mp3", format="mp3")
print("Audio editing complete. Edited file saved as 'edited_audio.mp3'.")

# Transcribe with Whisper
print("Transcribing audio...")
result = model.transcribe(
    audio_file,
    word_timestamps=True,
    # carry_initial_prompt=True,
    # initial_prompt="اااا فکر کنم که عااا نمدونم چرا عههه خوب میگم بهت الان ...",
)
print("Transcription complete.")

# with open('transcribe.txt', 'w', encoding='utf-8') as f:
#     f.write(result["text"])

words = result["segments"]

with open('transcribe.txt', 'w', encoding='utf-8') as f:
    for w in words:
        # f.write(w['text'] + " " + str(w['start']) + " " + str(w['end']) + "\n")
        f.write(json.dumps(w) + "\n")

allwords = []
for i, w in enumerate(words):
    for j, x in enumerate(w['words']):
        allwords.append({
            'text_idx': i,
            'word_idx': j,
            'word': x['word']
        })

# print(allwords)

class CorrectionWord(BaseModel):
    text_idx: int
    word_idx: int
    word: str
    is_correction: bool
    probability: float

class TaskCorrection(BaseModel):
    words: list[CorrectionWord]


print('Detecting spelling errors...')

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": """
You are provided by a transcription of an audio.
Correct the words that are misspelled or transcribed incorrectly.
Don't reponse anything other than the correction of the transcription.
        """},
        {"role": "user", "content": result["text"]},
    ],
)

event = completion.choices[0].message.content

print('Detection of spelling complete.')

with open('misspellings.txt', 'w', encoding='utf-8') as f:
    f.write(event)

print('Detecting correction words...')

completion = client.beta.chat.completions.parse(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": """
You are provided by text indexes and words of an audio oredered by their timestamps.
Set is_correction for the words that are spoken for the purpose of correction to their previous word.
Set probability for the level of confidence of is_correction.
        """},
        {"role": "user", "content": json.dumps(allwords)},
    ],
    response_format=TaskCorrection,
)

event = completion.choices[0].message.parsed

print('Detection of correction words complete.')

with open('corrections.txt', 'w', encoding='utf-8') as f:
    # def compare(x, y):
    #     diff = x.text_idx - y.text_idx
    #     if diff == 0:
    #         return x.word_idx - y.word_idx
    #     else:
    #         return diff
    # event.words.sort(key=lambda x: x.word_idx)

    prev_text_idx = -1
    for i, w in enumerate(event.words):
        try:
            if prev_text_idx != w.text_idx:
                prev_text_idx = w.text_idx
                f.write('sentence: ')
                f.write(words[w.text_idx]['text'])
                f.write('\n')
            if w.is_correction:
                f.write('word: ')
                f.write(w.word)
                f.write(' is correction for word: ')
                f.write(event.words[i - 1].word)
                f.write(' probability: ')
                f.write(str(w.probability))
                f.write('\n')
        except Exception as e:
            print(e)
